Sun Apr 28 16:04:28 EDT 2024
Your job is running on node(s):
adroit-h11g2
Working directory:
/scratch/network/hw8161/CoFiPruning/tasks/cola_mod_0.95
nvidia-smi output:
Sun Apr 28 16:04:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              34W / 250W |      2MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Layer 0, heads 5 6 7 8 9 10 11 pruned.
Layer 1, heads 3 4 5 6 7 8 9 10 11 pruned.
Layer 2, heads 3 4 5 6 7 8 9 10 11 pruned.
Layer 3, heads 3 4 5 6 7 8 9 10 11 pruned.
Layer 4, heads 9 10 11 pruned.
Layer 5, heads 11 pruned.
Layer 6, heads 8 9 10 11 pruned.
Layer 7, heads 7 8 9 10 11 pruned.
Layer 8, heads 4 5 6 7 8 9 10 11 pruned.
Layer 9, heads 4 5 6 7 8 9 10 11 pruned.
Layer 10, heads 3 4 5 6 7 8 9 10 11 pruned.
Layer 11, heads 9 10 11 pruned.
Layer: 0
query: torch.Size([320, 766])
key: torch.Size([320, 766])
value: torch.Size([320, 766])
output: torch.Size([766, 320])
up: torch.Size([1400, 766])
down: torch.Size([766, 1400])
Layer: 1
query: torch.Size([192, 766])
key: torch.Size([192, 766])
value: torch.Size([192, 766])
output: torch.Size([766, 192])
up: torch.Size([1396, 766])
down: torch.Size([766, 1396])
Layer: 2
query: torch.Size([192, 766])
key: torch.Size([192, 766])
value: torch.Size([192, 766])
output: torch.Size([766, 192])
up: torch.Size([1606, 766])
down: torch.Size([766, 1606])
Layer: 3
query: torch.Size([192, 766])
key: torch.Size([192, 766])
value: torch.Size([192, 766])
output: torch.Size([766, 192])
up: torch.Size([1549, 766])
down: torch.Size([766, 1549])
Layer: 4
query: torch.Size([576, 766])
key: torch.Size([576, 766])
value: torch.Size([576, 766])
output: torch.Size([766, 576])
up: torch.Size([1408, 766])
down: torch.Size([766, 1408])
Layer: 5
query: torch.Size([704, 766])
key: torch.Size([704, 766])
value: torch.Size([704, 766])
output: torch.Size([766, 704])
up: torch.Size([1263, 766])
down: torch.Size([766, 1263])
Layer: 6
query: torch.Size([512, 766])
key: torch.Size([512, 766])
value: torch.Size([512, 766])
output: torch.Size([766, 512])
up: torch.Size([894, 766])
down: torch.Size([766, 894])
Layer: 7
query: torch.Size([448, 766])
key: torch.Size([448, 766])
value: torch.Size([448, 766])
output: torch.Size([766, 448])
up: torch.Size([627, 766])
down: torch.Size([766, 627])
Layer: 8
query: torch.Size([256, 766])
key: torch.Size([256, 766])
value: torch.Size([256, 766])
output: torch.Size([766, 256])
up: torch.Size([523, 766])
down: torch.Size([766, 523])
Layer: 9
query: torch.Size([256, 766])
key: torch.Size([256, 766])
value: torch.Size([256, 766])
output: torch.Size([766, 256])
up: torch.Size([966, 766])
down: torch.Size([766, 966])
Layer: 10
query: torch.Size([192, 766])
key: torch.Size([192, 766])
value: torch.Size([192, 766])
output: torch.Size([766, 192])
up: torch.Size([516, 766])
down: torch.Size([766, 516])
Layer: 11
query: torch.Size([576, 766])
key: torch.Size([576, 766])
value: torch.Size([576, 766])
output: torch.Size([766, 576])
up: torch.Size([1840, 766])
down: torch.Size([766, 1840])
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 0: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 1: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 2: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 3: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 4: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 5: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 6: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 7: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 8: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 9: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 10: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 11: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 12: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 13: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 14: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 15: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 16: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 17: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 18: There are 9 batches in the dataset.
The following columns in the evaluation set  don't have a corresponding argument in `CoFiBertForSequenceClassification.forward` and have been ignored: sentence, idx.
Round 19: There are 9 batches in the dataset.
Task: cola
Model path: /scratch/network/hw8161/CoFiPruning/out/CoLA/CoFi_mod/CoLA_sparsity0.95/FT-lr3e-5
Model size: 35042628
Sparsity: 0.5880000000000001
matthews_correlation: 0.5291
seconds/example: 0.00124

